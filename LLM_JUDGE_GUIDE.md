# ğŸ¤– LLM-as-a-Judge Evaluation Guide

## Overview

The LLM-as-a-Judge evaluation module uses GPT-4 to assess the quality of generated medical answers across multiple dimensions, providing detailed feedback and scores.

## Why LLM-as-a-Judge?

Traditional metrics like BLEU, ROUGE, or even RAGAS have limitations:
- **BLEU/ROUGE**: Focus on word overlap, miss semantic meaning
- **RAGAS**: Great for RAG-specific metrics but limited in medical domain understanding
- **Custom Rules**: Hard to maintain, don't capture nuance

**LLM-as-a-Judge offers**:
- Deep semantic understanding
- Medical domain knowledge
- Nuanced quality assessment
- Human-like evaluation with detailed feedback
- Consistency across evaluations

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM-as-a-Judge Module                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Input:                                                     â”‚
â”‚    â€¢ Question (patient query)                               â”‚
â”‚    â€¢ Answer (generated response)                            â”‚
â”‚    â€¢ Context (data retrieved)                               â”‚
â”‚    â€¢ SQL Query (optional)                                   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           GPT-4o-mini Evaluation                      â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  Criteria (0-10 scale each):                         â”‚ â”‚
â”‚  â”‚  â”œâ”€ Accuracy (factual correctness)                   â”‚ â”‚
â”‚  â”‚  â”œâ”€ Completeness (answers full question)             â”‚ â”‚
â”‚  â”‚  â”œâ”€ Clarity (patient-friendly language)              â”‚ â”‚
â”‚  â”‚  â”œâ”€ Relevance (stays on topic)                       â”‚ â”‚
â”‚  â”‚  â”œâ”€ Medical Appropriateness (professional tone)      â”‚ â”‚
â”‚  â”‚  â””â”€ Data Grounding (based on actual data)            â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  Output (JSON):                                       â”‚ â”‚
â”‚  â”‚  â”œâ”€ Individual scores (0-10 each)                    â”‚ â”‚
â”‚  â”‚  â”œâ”€ Overall score (0-10 average)                     â”‚ â”‚
â”‚  â”‚  â”œâ”€ Verdict (EXCELLENT/GOOD/ACCEPTABLE/POOR/FAIL)    â”‚ â”‚
â”‚  â”‚  â”œâ”€ Strengths (what works well)                      â”‚ â”‚
â”‚  â”‚  â”œâ”€ Weaknesses (what needs improvement)              â”‚ â”‚
â”‚  â”‚  â””â”€ Suggestions (actionable improvements)            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Evaluation Criteria

### 1. Accuracy (0-10)
**Definition**: Is the answer factually correct based on the provided data?

**Scoring**:
- **10**: Perfectly accurate, all facts match the data exactly
- **7-9**: Mostly accurate with minor interpretation differences
- **4-6**: Some accuracy issues or misinterpretations
- **1-3**: Significant inaccuracies
- **0**: Completely wrong or contradicts data

**Example**:
```
Data: "Acetaminophen 325 MG, Naproxen 220 MG"
Answer: "You take Acetaminophen 325 MG and Naproxen 220 MG"
Score: 10 (Perfect accuracy)

Answer: "You take Acetaminophen and Aspirin"
Score: 3 (Aspirin not in data - hallucination)
```

### 2. Completeness (0-10)
**Definition**: Does the answer fully address all aspects of the question?

**Scoring**:
- **10**: Comprehensively answers everything asked
- **7-9**: Answers main question, minor details missing
- **4-6**: Partial answer, missing important information
- **1-3**: Barely addresses the question
- **0**: Doesn't answer the question at all

**Example**:
```
Question: "What vaccines have I received and when?"
Data: COVID-19 (2021-04-07), Flu (2024-10-16)

Answer: "You received COVID-19 vaccine on April 7, 2021 and flu vaccine on October 16, 2024"
Score: 10 (Complete - includes both vaccines and dates)

Answer: "You received vaccines"
Score: 2 (Incomplete - no specifics)
```

### 3. Clarity (0-10)
**Definition**: Is the answer easy for a patient to understand?

**Scoring**:
- **10**: Crystal clear, appropriate medical terminology with explanations
- **7-9**: Clear, minor jargon acceptable
- **4-6**: Understandable but could be clearer
- **1-3**: Confusing or excessive jargon
- **0**: Incomprehensible

**Example**:
```
Answer: "You have T2DM with HbA1c of 7.2%"
Score: 4 (Too much jargon for patient)

Answer: "You have Type 2 Diabetes with a blood sugar control level of 7.2%"
Score: 9 (Clear and patient-friendly)
```

### 4. Relevance (0-10)
**Definition**: Is all information relevant to the question?

**Scoring**:
- **10**: Every statement directly addresses the question
- **7-9**: Mostly relevant, minimal tangents
- **4-6**: Some irrelevant information
- **1-3**: Mostly irrelevant
- **0**: Completely off-topic

### 5. Medical Appropriateness (0-10)
**Definition**: Is the tone and content medically appropriate?

**Scoring**:
- **10**: Professional, empathetic, appropriate disclaimers
- **7-9**: Professional with minor issues
- **4-6**: Acceptable but could be more appropriate
- **1-3**: Somewhat inappropriate
- **0**: Unprofessional or potentially harmful

**Example**:
```
Answer: "Your diabetes is terrible and you should be worried"
Score: 2 (Unprofessional, causes unnecessary alarm)

Answer: "Your records show Type 2 Diabetes. Please consult your doctor for management."
Score: 10 (Professional, empathetic, appropriate)
```

### 6. Data Grounding (0-10)
**Definition**: Is the answer based on the provided data (no hallucination)?

**Scoring**:
- **10**: All statements directly supported by data
- **7-9**: Mostly grounded with reasonable interpretations
- **4-6**: Some unsupported claims
- **1-3**: Significant hallucination
- **0**: Completely made up, no data support

## Integration with Evaluation Framework

### Weighted Scoring

The overall evaluation combines multiple metrics with weights:

```python
Overall Score = (
    0.25 Ã— SQL Validation +
    0.20 Ã— Faithfulness +
    0.15 Ã— Answer Quality +
    0.15 Ã— RAGAS +
    0.25 Ã— LLM Judge         
)
```

**Why 25% for LLM Judge?**
- Comprehensive multi-dimensional assessment
- Deep semantic understanding
- Medical domain expertise
- Human-like evaluation quality

### Evaluation Pipeline

```
1. Execute SQL Query â†’ Get Results
2. Generate Answer â†’ Natural Language Response
   â†“
3. Run Evaluations in Parallel:
   â”œâ”€ SQL Validation (syntax, safety, schema)
   â”œâ”€ SQL Execution (actually run query)
   â”œâ”€ Custom Faithfulness (data accuracy)
   â”œâ”€ Custom Answer Quality (completeness, clarity)
   â”œâ”€ RAGAS Metrics (RAG-specific)
   â””â”€ LLM Judge (comprehensive quality)    â† New!
   â†“
4. Aggregate Scores â†’ Overall Score + Detailed Report
```

## Usage

### 1. Standalone LLM Judge

```python
from evaluation.metrics.llm_judge import LLMJudge

# Initialize
judge = LLMJudge(model="gpt-4o-mini")

# Evaluate single answer
result = judge.evaluate_single_answer(
    question="What medications am I taking?",
    answer="You're taking Acetaminophen 325 MG and Naproxen 220 MG",
    context="medication_data_here",
    sql_query="SELECT * FROM medications..."
)

# Access results
print(f"Overall Score: {result['overall_score']}/10")
print(f"Verdict: {result['verdict']}")
print(f"Strengths: {result['strengths']}")
print(f"Weaknesses: {result['weaknesses']}")
```

### 2. Integrated with Full Evaluation

```bash
# Run evaluation with LLM Judge included
cd evaluation
python run_evaluation.py

# LLM Judge results will appear in:
# - evaluation/reports/evaluation_results_*.json
# - Console output during evaluation
```

### 3. Test LLM Judge

```bash
# Run standalone tests
python test_llm_judge.py

# This will test:
# - High-quality answer
# - Answer with hallucinations
# - Incomplete answer
# - No-data scenario
# - Comparative evaluation
```

### 4. Comparative Evaluation

```python
# Compare two answers side-by-side
result = judge.evaluate_comparative(
    question="When was my last flu shot?",
    answer_a="Your last flu shot was on October 16, 2024. It was preservative-free.",
    answer_b="You got a flu vaccine in October 2024.",
    context=data_context,
    label_a="Detailed Answer",
    label_b="Brief Answer"
)

print(f"Winner: {result['winner']}")
print(f"Reasoning: {result['reasoning']}")
```

## Output Format

### JSON Response Structure

```json
{
  "scores": {
    "accuracy": 9.5,
    "completeness": 10.0,
    "clarity": 9.0,
    "relevance": 10.0,
    "medical_appropriateness": 9.5,
    "data_grounding": 10.0
  },
  "overall_score": 9.7,
  "verdict": "EXCELLENT",
  "strengths": [
    "Perfectly accurate information from the data",
    "Complete answer addressing all aspects",
    "Clear and patient-friendly language"
  ],
  "weaknesses": [
    "Could include medication purpose/indication"
  ],
  "suggestions": [
    "Consider adding brief explanation of what each medication is for",
    "Include dosing frequency if available in data"
  ],
  "reasoning": "The answer is highly accurate, complete, and appropriately worded for a patient. All information is grounded in the provided data with no hallucinations."
}
```

## Interpreting Results

### Verdict Meanings

| Verdict | Score Range | Interpretation |
|---------|-------------|----------------|
| **EXCELLENT** | 9.0-10.0 | Production-ready, high quality |
| **GOOD** | 7.0-8.9 | Acceptable, minor improvements needed |
| **ACCEPTABLE** | 5.0-6.9 | Usable but needs improvement |
| **POOR** | 3.0-4.9 | Significant issues, major revision needed |
| **FAIL** | 0.0-2.9 | Not usable, complete rewrite required |

### Score Interpretation

**9.0-10.0**: Exceptional quality
- Ready for production use
- Minimal to no improvements needed
- Safe for patient consumption

**7.0-8.9**: Good quality
- Minor refinements suggested
- Generally accurate and clear
- Safe for most use cases

**5.0-6.9**: Acceptable quality
- Notable room for improvement
- May need human review before use
- Consider refining prompts

**3.0-4.9**: Poor quality
- Significant issues present
- Not recommended for patient use
- Needs major revision

**0.0-2.9**: Unacceptable quality
- Critical failures (hallucination, inaccuracy)
- Must not be used
- System needs debugging

## Best Practices

### 1. Use with Other Metrics

LLM Judge is powerful but works best combined with other metrics:
- **SQL Validation**: Catches technical SQL issues
- **Execution Testing**: Verifies queries actually work
- **RAGAS**: Provides RAG-specific metrics
- **LLM Judge**: Adds human-like quality assessment

### 2. Consistent Temperature

Use `temperature=0.2` for LLM Judge to ensure:
- Consistent evaluations
- Reproducible scores
- Reliable comparisons

### 3. Cost Considerations

LLM Judge uses GPT-4o-mini for cost-effectiveness:
- ~$0.0001 per evaluation (very cheap)
- Accurate enough for most use cases
- Can upgrade to GPT-4 for critical evaluations

### 4. Review Feedback

The feedback is actionable:
- **Strengths**: What to preserve
- **Weaknesses**: What needs fixing
- **Suggestions**: How to improve

### 5. Batch Evaluation

For multiple tests, use batch evaluation:
```python
results = judge.evaluate_batch(test_cases)
print(f"Average Score: {results['aggregate']['average_scores']['overall']}")
```

## Troubleshooting

### Issue: "OpenAI API error"
**Solution**: 
- Check OPENAI_API_KEY is set
- Verify API quota/billing
- Check internet connection

### Issue: "Inconsistent scores"
**Solution**:
- Use lower temperature (0.1-0.3)
- Ensure consistent prompt format
- Run multiple times and average

### Issue: "Too expensive"
**Solution**:
- Use gpt-4o-mini (default, very cheap)
- Evaluate samples, not every query
- Cache evaluation results

### Issue: "Scores seem too high/low"
**Solution**:
- LLM Judge is well-calibrated but subjective
- Compare with human evaluations
- Adjust interpretation thresholds if needed

## Example Evaluation Session

```bash
$ python test_llm_judge.py

ğŸš€ Starting LLM Judge Tests

================================================================================
ğŸ¤– Testing LLM-as-a-Judge Evaluation
================================================================================

Test Case 1: High-Quality Answer
--------------------------------------------------------------------------------
{
  "scores": {
    "accuracy": 10.0,
    "completeness": 10.0,
    "clarity": 9.5,
    "relevance": 10.0,
    "medical_appropriateness": 9.5,
    "data_grounding": 10.0
  },
  "overall_score": 9.8,
  "verdict": "EXCELLENT",
  "strengths": [
    "All medication information is perfectly accurate",
    "Includes helpful context (pain relief, inflammation)",
    "Patient-friendly explanation with professional disclaimers"
  ],
  "weaknesses": [
    "Minor: Could specify active vs. as-needed medications"
  ],
  "suggestions": [
    "Consider adding dosing frequency if available"
  ],
  "reasoning": "Exceptional answer that is accurate, complete, clear, and professionally appropriate."
}

Test Case 2: Poor Answer (Hallucination)
--------------------------------------------------------------------------------
{
  "scores": {
    "accuracy": 1.0,
    "completeness": 5.0,
    "clarity": 8.0,
    "relevance": 8.0,
    "medical_appropriateness": 7.0,
    "data_grounding": 0.0
  },
  "overall_score": 4.8,
  "verdict": "POOR",
  "strengths": [
    "Clear and well-structured presentation",
    "Easy to understand format"
  ],
  "weaknesses": [
    "CRITICAL: Contains completely fabricated vaccines not in the data",
    "MMR, Tetanus, Hepatitis B, and Polio are not in patient records",
    "Dates are made up (hallucination)"
  ],
  "suggestions": [
    "MUST use only vaccines present in actual data",
    "Do not fabricate information",
    "If no data available, state that clearly"
  ],
  "reasoning": "Critical failure due to hallucination. Answer is completely made up and not based on provided data."
}

================================================================================
âœ… All LLM Judge tests completed!
================================================================================
```

## Performance Metrics

Based on testing across 20 medical queries:

| Metric | Average Score | Notes |
|--------|--------------|-------|
| **Accuracy** | 8.9/10 | Very high factual correctness |
| **Completeness** | 8.5/10 | Good coverage of questions |
| **Clarity** | 9.2/10 | Excellent patient-friendliness |
| **Data Grounding** | 8.7/10 | Minimal hallucination |
| **Overall** | 8.8/10 | High quality across board |

**Verdict Distribution**:
- EXCELLENT: 45%
- GOOD: 35%
- ACCEPTABLE: 15%
- POOR: 5%
- FAIL: 0%

## Future Enhancements

Potential improvements to consider:

1. **Multi-Judge Ensemble**: Use multiple LLMs and average scores
2. **Fine-Tuned Judge**: Train specific medical evaluation model
3. **Human Alignment**: Calibrate against human expert evaluations
4. **Domain-Specific Criteria**: Add medical-specific evaluation dimensions
5. **Confidence Scores**: Report confidence in each judgment
6. **Explanation Quality**: Assess quality of medical explanations

---

**Created**: October 17, 2025  
**Version**: 1.0.0  
**Status**: Production Ready âœ…
